{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d39585f",
   "metadata": {},
   "source": [
    "# Crypto Algorithmic Trading Strategy using RSI, Bollinger Bands & SMAs.\n",
    "\n",
    "---\n",
    "#### **Crypto Algorithmic Trading Strategy.**\n",
    "A machine learning-based trading model using **RSI, Bollinger Bands, XGBoost, SMAs**, and **Backtesting** for strategy evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Project Overview.\n",
    "This notebook presents an **algorithmic trading strategy** leveraging:\n",
    "- **Relative Strength Index (RSI)** for momentum analysis.\n",
    "- **Bollinger Bands** for volatility-based buy/sell signals.\n",
    "- **Simple Moving Averages** for trends-based signals.\n",
    "- **XGBoost** for machine learning-driven trade classification.\n",
    "\n",
    "The strategy also includes **risk management**, a **backtesting engine**, and **visualizations** using **Plotly**.\n",
    "\n",
    "## üöÄ Technologies Used.\n",
    "- **Python** üêç\n",
    "- **Jupyter Notebook**\n",
    "- **Binance API**\n",
    "- **TA-Lib (`ta` library)**\n",
    "- **Scikit-learn**\n",
    "- **XGBoost**\n",
    "- **Plotly**\n",
    "- **Request**\n",
    "- **Python-binance**\n",
    "- **Imblean or Imbalanced-learn**\n",
    "\n",
    "## üìä Trading Strategy Components.\n",
    "1Ô∏è‚É£ **Fetch Cryptocurrency Data** (Binance API)  \n",
    "2Ô∏è‚É£ **Compute Technical Indicators** (RSI, Bollinger Bands & SMAs)  \n",
    "3Ô∏è‚É£ **Generate Trading Labels** (Buy/Sell/Hold)  \n",
    "4Ô∏è‚É£ **Train ML Model** (XGBoost classifier)  \n",
    "5Ô∏è‚É£ **Backtest Trading Strategy** (Risk Management)  \n",
    "6Ô∏è‚É£ **Visualize Candlestick Charts with Bollinger Bands**  \n",
    "\n",
    "## üìÇ Dataset.\n",
    "- **Cryptocurencies (BTCUSDT/ETHUSDT/XRPUSDT, etc)** Price Data.\n",
    "--- 4h interval, 1000 limit. You can tweak this accordingly to your preference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37194f52",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Imports and Configuration\n",
    "\n",
    "This cell loads all necessary libraries, environment variables for API keys, and configures basic settings like logging and warning suppression.\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Python & Environment ---\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Data Handling & Numerics ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- API & Web ---\n",
    "import requests # Kept for general purpose, though Binance client is preferred for klines\n",
    "from requests.exceptions import ConnectionError, Timeout, HTTPError\n",
    "from binance.client import Client as BinanceClient # Using python-binance\n",
    "\n",
    "# --- Technical Analysis ---\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands \n",
    "from ta.trend import SMAIndicator # For SMAs\n",
    "\n",
    "# --- Machine Learning ---\n",
    "import xgboost as xgb # ML Model\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE # Make sure imbalanced-learn is installed\n",
    "\n",
    "# --- Plotting ---\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- Load Environment Variables (for API keys) ---\n",
    "load_dotenv()\n",
    "BINANCE_API_KEY = os.getenv(\"BINANCE_API_KEY\")\n",
    "BINANCE_SECRET_KEY = os.getenv(\"BINANCE_SECRET_KEY\")\n",
    "\n",
    "if not BINANCE_API_KEY or not BINANCE_SECRET_KEY:\n",
    "    print(\"‚ö†Ô∏è WARNING: Binance API keys not found in .env file or environment variables.\")\n",
    "    print(\"   The data fetching part might fail or use unauthenticated limits.\")\n",
    "    # For public data like klines, client can be initialized without keys, but it's good practice\n",
    "    binance_client = BinanceClient(None, None)\n",
    "else:\n",
    "    binance_client = BinanceClient(BINANCE_API_KEY, BINANCE_SECRET_KEY)\n",
    "\n",
    "# --- Configure Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s')\n",
    "\n",
    "# --- Suppress Specific Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "print(\"‚úÖ Setup Complete: Libraries imported and configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673498fd",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Imports and Configuration\n",
    "\n",
    "This cell loads all necessary libraries, environment variables for API keys, and configures basic settings like logging and warning suppression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_crypto_data_client(client, symbol=\"BTCUSDT\", interval=\"4h\", limit=1000, start_str=\"1 Jan, 2021\", end_str=\"1 Jul, 2025\"):\n",
    "    \"\"\"\n",
    "     Fetch historical candlestick data using python-binance client.\n",
    "     Can fetch by limit (most recent) or by date range.\n",
    "\n",
    "     Parameters:\n",
    "       - client: Initialized BinanceAPI client.\n",
    "       - symbol (str): Trading pair (e.g., 'BTCUSDT').\n",
    "       - interval (str): Candlestick interval (e.g., '1m', '1h', '1d').\n",
    "       - limit (int): Number of candles to fetch if start_str is None.\n",
    "       - start_str (str, optional): Start date string (e.g., \"1 Jan, 2020\").\n",
    "       - end_str (str, optional): End date string (e.g., \"1 Jan, 2021\").\n",
    "\n",
    "     Returns:\n",
    "       DataFrame: Candlestick data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Fetching data for {symbol}, interval {interval}, limit {limit}, start '{start_str}', end '{end_str}'\")\n",
    "    try:\n",
    "        if start_str:\n",
    "            klines = client.get_historical_klines(symbol, interval, start_str, end_str=end_str)\n",
    "            logging.info(f\"Fetched {len(klines)} klines from {start_str} to {end_str or 'now'}\")\n",
    "        else:\n",
    "            klines = client.get_historical_klines(symbol, interval, limit=limit)\n",
    "            logging.info(f\"Fetched {len(klines)} (limit: {limit}) most recent klines.\")\n",
    "\n",
    "        if not klines:\n",
    "            logging.warning(\"No klines data returned from Binance.\")\n",
    "            return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    except Exception as e: # Catches BinanceAPIException, BinanceRequestException, etc.\n",
    "        logging.error(f\"Error fetching klines from Binance: {e}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "        \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "        \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    "    ])\n",
    "\n",
    "    # Convert timestamp columns\n",
    "    df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit=\"ms\") # Binance provides UTC\n",
    "    df[\"close_time\"] = pd.to_datetime(df[\"close_time\"], unit=\"ms\")\n",
    "    df[\"timestamp\"] = df[\"open_time\"] # Use open_time as the primary timestamp for the bar\n",
    "\n",
    "    # Convert numeric columns\n",
    "    numeric_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"quote_asset_volume\",\n",
    "                    \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    logging.info(f\"Data fetched successfully. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# --- Example Usage (Comment out if running the full pipeline later in main) ---\n",
    "test_df = fetch_crypto_data_client(binance_client, symbol=\"BTCHUSDT\", interval=\"4h\", limit=1000, start_str=\"1 Jan, 2021\", end_str=\"1 Jul, 2025\")\n",
    "if test_df is not None and not test_df.empty:\n",
    "    print(\"\\n--- Fetched Data Sample ---\")\n",
    "    print(test_df.head())\n",
    "    print(f\"\\nData types:\\n{test_df.dtypes}\")\n",
    "else:\n",
    "    print(\"Failed to fetch test data or data is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b49d3",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Feature Engineering Functions\n",
    "\n",
    "These functions calculate technical indicators like RSI, Bollinger Bands, and SMAs using the `ta` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9181650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_technical_indicators(df, rsi_window=14, bb_window=20, bb_std_dev=2,\n",
    "                                   sma_short_window=50, sma_long_window=200):\n",
    "    \"\"\"\n",
    "    Calculate RSI, Bollinger Bands, SMAs, and basic volume analysis.\n",
    "    Handles NaNs by filling or they will be dropped later.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Calculating technical indicators. Initial df shape: {df.shape}\")\n",
    "    if df.empty or 'close' not in df.columns or len(df) < max(rsi_window, bb_window, sma_long_window): # Ensure enough data\n",
    "        logging.warning(\"DataFrame is empty or too short for feature calculation.\")\n",
    "        # Add empty columns if df is not empty but too short, to maintain structure\n",
    "        if not df.empty:\n",
    "            for col in [\"RSI\", \"BB_mavg\", \"BB_hband\", \"BB_lband\", \"SMA_short\", \"SMA_long\", \"volume_pct_change\"]:\n",
    "                if col not in df.columns: df[col] = np.nan\n",
    "        return df\n",
    "\n",
    "    # RSI\n",
    "    rsi_indicator = RSIIndicator(close=df[\"close\"], window=rsi_window, fillna=True) # fillna handles initial NaNs\n",
    "    df[\"RSI\"] = rsi_indicator.rsi()\n",
    "    logging.info(\"RSI calculated.\")\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bb_indicator = BollingerBands(close=df[\"close\"], window=bb_window, window_dev=bb_std_dev, fillna=True)\n",
    "    df[\"BB_mavg\"] = bb_indicator.bollinger_mavg()   # Middle Band\n",
    "    df[\"BB_hband\"] = bb_indicator.bollinger_hband() # Upper Band\n",
    "    df[\"BB_lband\"] = bb_indicator.bollinger_lband() # Lower Band\n",
    "    logging.info(\"Bollinger Bands calculated.\")\n",
    "\n",
    "    # SMAs\n",
    "    sma_short_indicator = SMAIndicator(close=df[\"close\"], window=sma_short_window, fillna=True)\n",
    "    df[\"SMA_short\"] = sma_short_indicator.sma_indicator()\n",
    "    \n",
    "    sma_long_indicator = SMAIndicator(close=df[\"close\"], window=sma_long_window, fillna=True)\n",
    "    df[\"SMA_long\"] = sma_long_indicator.sma_indicator()\n",
    "    logging.info(\"SMAs calculated.\")\n",
    "\n",
    "    # Volume Percentage Change\n",
    "    df[\"volume_pct_change\"] = df[\"volume\"].pct_change(fill_method=None) # Default fillna later\n",
    "    df[\"volume_pct_change\"].fillna(0, inplace=True) # Fill first NaN with 0\n",
    "    logging.info(\"Volume percentage change calculated.\")\n",
    "    \n",
    "    logging.info(f\"Technical indicators calculation complete. df shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# --- Example Usage (Comment out if running the full pipeline later in main) ---\n",
    "if 'test_df' in globals() and test_df is not None and not test_df.empty:\n",
    "    test_df_features = calculate_technical_indicators(test_df.copy())\n",
    "    print(\"\\n--- Data Sample with Features ---\")\n",
    "    print(test_df_features[[\"timestamp\", \"close\", \"RSI\", \"BB_mavg\", \"BB_hband\", \"BB_lband\", \"SMA_short\", \"SMA_long\"]].tail())\n",
    "    print(f\"\\nNaNs after feature calculation:\\n{test_df_features.isnull().sum()}\")\n",
    "else:\n",
    "    print(\"Skipping feature calculation example as test_df is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa077e",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Target Variable Generation\n",
    "This section defines functions to generate target labels based on RSI quantiles.\n",
    "Crucially, thresholds are derived *only* from the training set's RSI distribution and then applied to both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aaf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsi_quantile_thresholds(rsi_series_train, lower_quantile=0.10, upper_quantile=0.90):\n",
    "    \"\"\"\n",
    "    Calculates RSI buy/sell thresholds based on quantiles of a given RSI series (training data).\n",
    "    \"\"\"\n",
    "    if rsi_series_train.empty or rsi_series_train.isnull().all():\n",
    "        logging.warning(\"RSI series for threshold calculation is empty or all NaN. Using default thresholds.\")\n",
    "        return 35, 65 # Default fallback thresholds\n",
    "\n",
    "    buy_threshold = rsi_series_train.quantile(lower_quantile)\n",
    "    sell_threshold = rsi_series_train.quantile(upper_quantile)\n",
    "    \n",
    "    # Ensure buy_threshold is less than sell_threshold\n",
    "    if buy_threshold >= sell_threshold:\n",
    "        logging.warning(f\"Calculated buy_threshold ({buy_threshold:.2f}) >= sell_threshold ({sell_threshold:.2f}). Adjusting or using defaults.\")\n",
    "        # Attempt to adjust, or fall back to ensure separation\n",
    "        if sell_threshold < 50 : buy_threshold = max(10, sell_threshold - 5) # Ensure some gap\n",
    "        elif buy_threshold > 50 : sell_threshold = min(90, buy_threshold + 5)\n",
    "        else: # If they are very close or crossed around 50, use defaults\n",
    "            buy_threshold, sell_threshold = 35, 65\n",
    "            logging.warning(\"Using default thresholds 35/65 due to quantile overlap.\")\n",
    "\n",
    "\n",
    "    logging.info(f\"RSI Thresholds calculated from training data: Buy <= {buy_threshold:.2f}, Sell >= {sell_threshold:.2f}\")\n",
    "    return buy_threshold, sell_threshold\n",
    "\n",
    "def apply_rsi_labels_to_series(rsi_series, buy_threshold, sell_threshold):\n",
    "    \"\"\"\n",
    "    Applies target labels (0: Hold, 1: Buy, 2: Sell) to an RSI series based on pre-calculated thresholds.\n",
    "    \"\"\"\n",
    "    # Buy Signal (1): RSI is below or equal to the buy_threshold.\n",
    "    # Sell Signal (2): RSI is above or equal to the sell_threshold.\n",
    "    # Neutral (0): Otherwise.\n",
    "    target_labels = np.select(\n",
    "        [rsi_series <= buy_threshold, rsi_series >= sell_threshold], # Conditions\n",
    "        [1, 2],                                                      # Choices for Buy, Sell\n",
    "        default=0                                                    # Default is Hold\n",
    "    )\n",
    "    return target_labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300aef0",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Data Preparation for Machine Learning\n",
    "\n",
    "This involves:\n",
    "1.  Defining the feature set (`X`).\n",
    "2.  Handling any remaining NaNs after feature engineering (especially relevant if `fillna=False` was used in `ta` or for initial rolling window periods).\n",
    "3.  Splitting data into training and testing sets chronologically (`shuffle=False`).\n",
    "4.  Generating `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(df_featurized, model_input_features, target_rsi_series_name=\"RSI\",\n",
    "                    test_set_size=0.2, random_state_split=42, # random_state_split is not used if shuffle=False, stratify=None\n",
    "                    stratify_on_rsi_quantiles=True, # This argument will now be ignored if shuffle=False\n",
    "                    lower_quantile=0.20, upper_quantile=0.80):\n",
    "    \"\"\"\n",
    "    Prepares features (X) and target (y) for ML, splits into train/test, and applies correct labeling.\n",
    "    For time-series (shuffle=False), stratification is disabled.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Preparing ML data. Input featurized df shape: {df_featurized.shape}\")\n",
    "    logging.info(f\"Using lower_quantile: {lower_quantile}, upper_quantile: {upper_quantile} for RSI thresholds.\")\n",
    "\n",
    "    # --- 1. Drop NaNs from rows based on selected model input features & target RSI series ---\n",
    "    cols_to_check_for_nan = model_input_features + [target_rsi_series_name]\n",
    "    df_cleaned = df_featurized.dropna(subset=cols_to_check_for_nan).copy()\n",
    "    \n",
    "    if df_cleaned.empty:\n",
    "        logging.error(\"DataFrame is empty after dropping NaNs from critical ML columns. Cannot proceed.\")\n",
    "        return None, None, None, None, None \n",
    "\n",
    "    logging.info(f\"DataFrame shape after NaN drop from critical columns: {df_cleaned.shape}\")\n",
    "\n",
    "    # --- 2. Define X (features) and the RSI series for labeling ---\n",
    "    X_full = df_cleaned[model_input_features]\n",
    "    rsi_for_labeling_full = df_cleaned[target_rsi_series_name]\n",
    "\n",
    "    if len(X_full) < 20: \n",
    "        logging.error(f\"Not enough data ({len(X_full)} rows) for a meaningful train/test split. Need at least 20.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # --- 3. Split Data (Chronological for Time Series) ---\n",
    "    if stratify_on_rsi_quantiles:\n",
    "        logging.warning(\"Stratification is requested but shuffle=False. Stratification will be disabled for train_test_split.\")\n",
    "\n",
    "    X_train, X_test, rsi_train, rsi_test = train_test_split(\n",
    "        X_full, rsi_for_labeling_full,\n",
    "        test_size=test_set_size,\n",
    "        shuffle=False, \n",
    "        stratify=None, # MUST BE NONE if shuffle=False\n",
    "        random_state=None # Not used if shuffle=False and stratify=None\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Data split: X_train shape {X_train.shape}, X_test shape {X_test.shape}\")\n",
    "\n",
    "    # --- 4. Generate Target Labels Correctly (Post-Split) ---\n",
    "    # Ensure rsi_train is not empty before proceeding (can happen if test_size is too large or data too small)\n",
    "    if rsi_train.empty:\n",
    "        logging.error(\"rsi_train is empty after split. Cannot generate labels. Check data size and test_set_size.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    buy_thresh_train, sell_thresh_train = get_rsi_quantile_thresholds(\n",
    "        rsi_train,\n",
    "        lower_quantile=lower_quantile, \n",
    "        upper_quantile=upper_quantile  \n",
    "    )\n",
    "    \n",
    "    y_train = apply_rsi_labels_to_series(rsi_train, buy_thresh_train, sell_thresh_train)\n",
    "    y_test = apply_rsi_labels_to_series(rsi_test, buy_thresh_train, sell_thresh_train) \n",
    "    \n",
    "    logging.info(f\"Target labels generated. y_train distribution: {dict(pd.Series(y_train).value_counts())}\")\n",
    "    logging.info(f\"y_test distribution: {dict(pd.Series(y_test).value_counts())}\")\n",
    "    \n",
    "    train_thresholds_info = {'buy_threshold': buy_thresh_train, 'sell_threshold': sell_thresh_train}\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, train_thresholds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f24ada",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ SMOTE for Handling Class Imbalance (Applied to Training Data Only)\n",
    "\n",
    "If class imbalance is an issue in `y_train`, SMOTE can be used to oversample minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96408ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote_to_train_data(X_train_orig, y_train_orig, random_state_smote=42):\n",
    "    \"\"\"Applies SMOTE to the training data if conditions are met.\"\"\"\n",
    "    logging.info(f\"Attempting SMOTE. Original y_train distribution: {dict(pd.Series(y_train_orig).value_counts())}\")\n",
    "    \n",
    "    X_train_resampled = X_train_orig.copy()\n",
    "    y_train_resampled = y_train_orig.copy()\n",
    "\n",
    "    unique_classes, counts = np.unique(y_train_orig, return_counts=True)\n",
    "\n",
    "    if len(unique_classes) < 2:\n",
    "        logging.warning(\"Only one class in y_train. SMOTE not applicable.\")\n",
    "        return X_train_resampled, y_train_resampled\n",
    "    \n",
    "    min_class_count = min(counts)\n",
    "    # k_neighbors for SMOTE must be less than the number of samples in the smallest class\n",
    "    # So, if smallest class has M samples, k_neighbors <= M-1.\n",
    "    # SMOTE also requires at least 2 samples for its default k_neighbors=5, or k_neighbors+1 samples.\n",
    "    \n",
    "    if min_class_count <= 1: # If any class has only 1 sample, k_neighbors cannot be >= 1.\n",
    "        logging.warning(f\"Smallest class has {min_class_count} samples. SMOTE requires k_neighbors+1 samples per class. Skipping SMOTE.\")\n",
    "        return X_train_resampled, y_train_resampled\n",
    "\n",
    "    # Determine a safe k_neighbors\n",
    "    k_neighbors_val = min(5, min_class_count - 1) # Cap at 5, ensure it's at least 1\n",
    "\n",
    "    if k_neighbors_val < 1:\n",
    "        logging.warning(f\"Calculated k_neighbors ({k_neighbors_val}) is < 1. Smallest class count: {min_class_count}. Skipping SMOTE.\")\n",
    "        return X_train_resampled, y_train_resampled\n",
    "        \n",
    "    try:\n",
    "        logging.info(f\"Applying SMOTE with k_neighbors={k_neighbors_val}\")\n",
    "        smote = SMOTE(random_state=random_state_smote, k_neighbors=k_neighbors_val)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_orig, y_train_orig)\n",
    "        logging.info(f\"SMOTE applied. Resampled y_train distribution: {dict(pd.Series(y_train_resampled).value_counts())}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error during SMOTE: {e}. Smallest class count: {min_class_count}, k_neighbors: {k_neighbors_val}. Using original data.\")\n",
    "    except ImportError:\n",
    "        logging.error(\"imblearn library not found. SMOTE cannot be applied. Using original data.\")\n",
    "        \n",
    "    return X_train_resampled, y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0fc973",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ XGBoost Model Training Function\n",
    "\n",
    "This function encapsulates the two-stage hyperparameter tuning (RandomizedSearchCV then GridSearchCV) and training for an XGBoost classifier. It also handles class weighting, which can be used alternatively or supplementarily to SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe77a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X_train_data, y_train_data, n_random_iter=25, use_sample_weights=True):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier with hyperparameter tuning.\n",
    "    If use_sample_weights is True, computes and applies class weights.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Starting XGBoost Training --- Input X shape: {X_train_data.shape}, y dist: {dict(pd.Series(y_train_data).value_counts())}\")\n",
    "\n",
    "    fit_params = {}\n",
    "    if use_sample_weights:\n",
    "        unique_classes_train = np.unique(y_train_data)\n",
    "        if len(unique_classes_train) > 1 : # Ensure multiple classes for weighting\n",
    "            class_weights_values = compute_class_weight(\"balanced\", classes=unique_classes_train, y=y_train_data)\n",
    "            weights_dict = dict(zip(unique_classes_train, class_weights_values))\n",
    "            sample_weight_array = np.array([weights_dict[val] for val in y_train_data])\n",
    "            fit_params['sample_weight'] = sample_weight_array\n",
    "            logging.info(f\"Sample weights computed and will be used. Weights dict: {weights_dict}\")\n",
    "        else:\n",
    "            logging.warning(\"Only one class in y_train_data. Skipping sample weight calculation.\")\n",
    "    \n",
    "    xgb_clf_base = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False, # Deprecated, use_label_encoder=False is default in newer versions\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # --- Stage 1: RandomizedSearchCV ---\n",
    "    logging.info(\"--- Stage 1: RandomizedSearchCV ---\")\n",
    "    param_dist = {\n",
    "        'n_estimators': [200, 300, 500], # Reduced for faster example\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2], # Added gamma for regularization\n",
    "        'reg_alpha': [0, 0.01, 0.1, 0.5],\n",
    "        'reg_lambda': [0.5, 1.0, 1.5]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_clf_base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_random_iter,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42), # Using StratifiedKFold\n",
    "        scoring='f1_macro',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        random_search.fit(X_train_data, y_train_data, **fit_params)\n",
    "        logging.info(f\"RandomizedSearchCV Best F1 Macro: {random_search.best_score_:.4f}\")\n",
    "        logging.info(f\"RandomizedSearchCV Best Params: {random_search.best_params_}\")\n",
    "        best_params_rs = random_search.best_params_\n",
    "    except Exception as e_rs:\n",
    "        logging.error(f\"RandomizedSearchCV failed: {e_rs}. Returning base model.\")\n",
    "        # Fallback: Fit a base model with default-ish params if search fails\n",
    "        xgb_clf_base.fit(X_train_data, y_train_data, **fit_params)\n",
    "        return xgb_clf_base\n",
    "\n",
    "\n",
    "    # --- Stage 2: GridSearchCV (Refined Search) ---\n",
    "    logging.info(\"--- Stage 2: GridSearchCV (Refined Search) ---\")\n",
    "    \n",
    "    # Refined grid helper (simplified version for brevity, your previous one was more detailed)\n",
    "    def get_refined_param_grid(best_params, base_param_dist):\n",
    "        refined_grid = {}\n",
    "        for param, value in best_params.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                if param in ['n_estimators', 'max_depth']: # Discrete steps\n",
    "                    original_options = sorted(list(set(base_param_dist[param])))\n",
    "                    current_idx = original_options.index(value) if value in original_options else -1\n",
    "                    low_idx = max(0, current_idx -1)\n",
    "                    high_idx = min(len(original_options)-1, current_idx + 1)\n",
    "                    options = [original_options[low_idx], value, original_options[high_idx]]\n",
    "                    refined_grid[param] = sorted(list(set(o for o in options if o is not None)))\n",
    "\n",
    "                elif isinstance(value, float): # Continuous, tighter range\n",
    "                     refined_grid[param] = sorted(list(set(np.clip([value * 0.8, value, value * 1.2], min(base_param_dist[param]), max(base_param_dist[param])))))\n",
    "                else: # int but not estimators/depth\n",
    "                    refined_grid[param] = [max(0, value -1), value, value+1] if value > 0 else [0,1,2]\n",
    "            else:\n",
    "                refined_grid[param] = [value] # Keep best or define a small range\n",
    "        # Ensure essential params are present even if not optimized by RS (if they were fixed)\n",
    "        for p in ['subsample', 'colsample_bytree', 'gamma', 'reg_alpha', 'reg_lambda']:\n",
    "            if p not in refined_grid: refined_grid[p] = [best_params.get(p, base_param_dist[p][0])] # fallback\n",
    "        return refined_grid\n",
    "\n",
    "    param_grid_refined = get_refined_param_grid(best_params_rs, param_dist)\n",
    "    logging.info(f\"Refined Parameter Grid for GridSearchCV: {param_grid_refined}\")\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_clf_base, # Important: use the base estimator\n",
    "        param_grid=param_grid_refined,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        grid_search.fit(X_train_data, y_train_data, **fit_params)\n",
    "        logging.info(f\"GridSearchCV Best F1 Macro: {grid_search.best_score_:.4f}\")\n",
    "        logging.info(f\"GridSearchCV Best Params: {grid_search.best_params_}\")\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e_gs:\n",
    "        logging.error(f\"GridSearchCV failed: {e_gs}. Using best model from RandomizedSearch or base if that also failed.\")\n",
    "        best_model = random_search.best_estimator_ # Fallback to RS best\n",
    "\n",
    "    logging.info(\"--- XGBoost Training Finished ---\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ef603",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Model Evaluation Functions\n",
    "\n",
    "Functions to evaluate the trained model's performance using various metrics, including cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ml_model(model_to_eval, X_eval_data, y_eval_data, dataset_name=\"Test\"):\n",
    "    \"\"\"Evaluates model on a given dataset.\"\"\"\n",
    "    if X_eval_data.empty or len(y_eval_data) == 0:\n",
    "        logging.warning(f\"Evaluation data for {dataset_name} is empty. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"--- Evaluating Model on {dataset_name} Set ---\")\n",
    "    try:\n",
    "        predictions = model_to_eval.predict(X_eval_data)\n",
    "        proba_predictions = model_to_eval.predict_proba(X_eval_data) # For more detailed analysis if needed\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during prediction on {dataset_name} set: {e}\")\n",
    "        return\n",
    "\n",
    "    acc = accuracy_score(y_eval_data, predictions)\n",
    "    logging.info(f\"‚úÖ {dataset_name} Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    report_labels = sorted(np.unique(np.concatenate((y_eval_data, predictions))))\n",
    "    if not report_labels: report_labels = None # Handle case of no labels\n",
    "\n",
    "    class_report = classification_report(y_eval_data, predictions, labels=report_labels, zero_division=0, target_names=[f\"Class {l}\" for l in report_labels] if report_labels else None)\n",
    "    logging.info(f\"üìä {dataset_name} Classification Report:\\n{class_report}\")\n",
    "    \n",
    "    if report_labels and len(report_labels) > 1: # Confusion matrix needs at least 2 classes typically\n",
    "        cm = confusion_matrix(y_eval_data, predictions, labels=report_labels)\n",
    "        logging.info(f\"üü¶ {dataset_name} Confusion Matrix (labels: {report_labels}):\\n{cm}\")\n",
    "    elif report_labels:\n",
    "         logging.info(f\"Only one class ({report_labels[0]}) present in y_eval_data/predictions for {dataset_name}. Confusion matrix not meaningful.\")\n",
    "\n",
    "\n",
    "def advanced_ml_evaluation(model_to_eval, X_train_data_cv, y_train_data_cv, cv_folds=3):\n",
    "    \"\"\"Performs cross-validation and full training set evaluation.\"\"\"\n",
    "    if X_train_data_cv.empty or len(y_train_data_cv) == 0:\n",
    "        logging.warning(\"Training data for advanced evaluation is empty. Skipping.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"\\n--- Advanced Evaluation Metrics (on Training Data variants) ---\")\n",
    "    \n",
    "    # --- Cross-Validation ---\n",
    "    # Note: Uses the original X_train_data_cv, y_train_data_cv (could be SMOTE'd or original)\n",
    "    # CV might not use sample_weights from the initial fit unless model_to_eval inherently carries them\n",
    "    # or fit_params are passed to cross_val_score. XGBoost objects don't typically store sample_weight after fit.\n",
    "    # For this demo, we evaluate the final model_to_eval's generalizability on folds of X_train_data_cv.\n",
    "    unique_cv_labels = np.unique(y_train_data_cv)\n",
    "    if len(unique_cv_labels) < 2 :\n",
    "        logging.warning(\"Not enough classes in y_train_data_cv for stratified cross-validation. Skipping CV.\")\n",
    "    else:\n",
    "        logging.info(f\"‚ö° Cross-Validation Metrics (using {cv_folds}-fold StratifiedKFold):\")\n",
    "        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        scoring_metrics = {'accuracy': 'accuracy', 'precision_macro': 'precision_macro',\n",
    "                           'recall_macro': 'recall_macro', 'f1_macro': 'f1_macro'}\n",
    "        cv_results = {}\n",
    "        for metric_name, scorer_name in scoring_metrics.items():\n",
    "            try:\n",
    "                # Ensure enough samples per class for each split for some scorers\n",
    "                cv_score = cross_val_score(model_to_eval, X_train_data_cv, y_train_data_cv, cv=skf, scoring=scorer_name)\n",
    "                cv_results[metric_name] = (cv_score.mean(), cv_score.std())\n",
    "                logging.info(f\"‚úî CV {metric_name.replace('_', ' ').title()}: {cv_score.mean():.4f} ¬± {cv_score.std():.4f}\")\n",
    "            except ValueError as e_cv:\n",
    "                 logging.warning(f\"Could not compute CV for {metric_name} (e.g., due to class distribution in a fold): {e_cv}\")\n",
    "                 cv_results[metric_name] = (np.nan, np.nan)\n",
    "\n",
    "\n",
    "    # --- Performance on Full Training Data (Sanity Check) ---\n",
    "    logging.info(\"\\nüìä Performance on Full Training Data used for this evaluation (Sanity Check):\")\n",
    "    # This evaluates how well the model fits the data it was trained on (or the processed version passed here)\n",
    "    # The model_to_eval is already fitted.\n",
    "    try:\n",
    "        preds_full_train = model_to_eval.predict(X_train_data_cv)\n",
    "        report_labels_train = sorted(np.unique(np.concatenate((y_train_data_cv, preds_full_train))))\n",
    "        if not report_labels_train: report_labels_train = None\n",
    "\n",
    "        full_train_report = classification_report(y_train_data_cv, preds_full_train, labels=report_labels_train, zero_division=0, target_names=[f\"Class {l}\" for l in report_labels_train] if report_labels_train else None)\n",
    "        logging.info(f\"Classification Report (Full Training Data variant):\\n{full_train_report}\")\n",
    "\n",
    "        if report_labels_train and len(report_labels_train) > 1:\n",
    "            cm_full_train = confusion_matrix(y_train_data_cv, preds_full_train, labels=report_labels_train)\n",
    "            logging.info(f\"Confusion Matrix (Full Training Data variant - labels: {report_labels_train}):\\n{cm_full_train}\")\n",
    "        elif report_labels_train:\n",
    "            logging.info(f\"Only one class ({report_labels_train[0]}) present for Full Training Data variant. CM not meaningful.\")\n",
    "\n",
    "    except Exception as e_eval_full:\n",
    "        logging.error(f\"Error evaluating on full training data variant: {e_eval_full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2bff2e",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Apply Trained Model to DataFrame\n",
    "\n",
    "This function takes a trained model and a DataFrame (which should have the necessary features) and adds a `predicted_signal` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2245053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_trained_model_to_df(df_input, trained_ml_model, model_features_list):\n",
    "    \"\"\"\n",
    "    Applies a trained ML model to generate predictions on a DataFrame.\n",
    "    Handles NaNs in prediction features by creating NaNs in predicted_signal.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Applying trained model to DataFrame. Input shape: {df_input.shape}\")\n",
    "    df_with_predictions = df_input.copy()\n",
    "    \n",
    "    # Ensure all required model features are present\n",
    "    missing_cols = [col for col in model_features_list if col not in df_with_predictions.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"DataFrame missing required features for prediction: {missing_cols}. Cannot apply model.\")\n",
    "        df_with_predictions[\"predicted_signal\"] = np.nan # Add NaN column\n",
    "        return df_with_predictions\n",
    "\n",
    "    X_to_predict = df_with_predictions[model_features_list].copy()\n",
    "    \n",
    "    # Initialize predicted_signal column with NaN\n",
    "    df_with_predictions[\"predicted_signal\"] = np.nan\n",
    "    \n",
    "    # Identify rows that are valid for prediction (all features are non-NaN)\n",
    "    valid_rows_mask = X_to_predict.notna().all(axis=1)\n",
    "    X_valid_to_predict = X_to_predict[valid_rows_mask]\n",
    "\n",
    "    if X_valid_to_predict.empty:\n",
    "        logging.warning(\"No valid rows with all required features non-NaN. No predictions will be made.\")\n",
    "    else:\n",
    "        logging.info(f\"Making predictions on {len(X_valid_to_predict)} valid rows.\")\n",
    "        try:\n",
    "            predictions_array = trained_ml_model.predict(X_valid_to_predict)\n",
    "            # Assign predictions only to the valid rows using their original index\n",
    "            df_with_predictions.loc[X_valid_to_predict.index, \"predicted_signal\"] = predictions_array\n",
    "            logging.info(\"Predictions applied successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during model prediction: {e}. 'predicted_signal' will remain NaN for these rows.\")\n",
    "            \n",
    "    return df_with_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c69f7",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£0Ô∏è‚É£ Backtesting Function (ML Strategy Version)\n",
    "\n",
    "This function simulates trading based on the `predicted_signal` from the ML model and other defined entry/exit rules. It includes fees, slippage, and risk management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83746af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the more detailed backtest_ml_strategy function\n",
    "def backtest_ml_strategy(df_with_signals, initial_balance=10000, fee_rate=0.001, slippage_rate=0.0005,\n",
    "                         risk_fraction_per_trade=0.02, stop_loss_pct=0.02, take_profit_pct=0.04,\n",
    "                         signal_col=\"predicted_signal\", rsi_col=\"RSI\", bb_lower_col=\"BB_lband\",\n",
    "                         sma_short_col=\"SMA_short\", sma_long_col=\"SMA_long\"):\n",
    "    \"\"\"\n",
    "    Simulate a backtest of a trading strategy based on ML model signals.\n",
    "    Assumes df_with_signals has 'close', 'low', 'high', and all specified indicator/signal columns.\n",
    "    \"\"\"\n",
    "    balance = initial_balance\n",
    "    position_qty = 0\n",
    "    \n",
    "    num_wins = 0\n",
    "    num_losses = 0\n",
    "    total_pnl_realized = 0 # Sum of P&L from closed trades\n",
    "    \n",
    "    balance_history = [initial_balance]\n",
    "    trades_log = [] # To log individual trades\n",
    "    active_trade = None \n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols_backtest = ['timestamp', 'open', 'high', 'low', 'close', signal_col, rsi_col, bb_lower_col, sma_short_col, sma_long_col]\n",
    "    missing_backtest_cols = [col for col in required_cols_backtest if col not in df_with_signals.columns]\n",
    "    if missing_backtest_cols:\n",
    "        logging.error(f\"DataFrame for backtest is missing required columns: {missing_backtest_cols}. Aborting backtest.\")\n",
    "        return 0, {\"error\": \"Missing columns\"}, []\n",
    "\n",
    "    logging.info(f\"--- Starting Backtest --- Initial Balance: ${initial_balance:.2f}\")\n",
    "    df_backtest = df_with_signals.copy() # Work on a copy\n",
    "\n",
    "    for idx, row in df_backtest.iterrows():\n",
    "        current_dt = row[\"timestamp\"]\n",
    "        current_price_close = row[\"close\"]\n",
    "        current_price_low = row[\"low\"]\n",
    "        current_price_high = row[\"high\"]\n",
    "\n",
    "        # --- Manage Active Trade (Check Exits First) ---\n",
    "        if active_trade:\n",
    "            trade_closed_this_bar = False\n",
    "            # Trailing Stop (Example: Simple percentage based trail from high since entry)\n",
    "            if active_trade['type'] == 'long':\n",
    "                # More robust trailing stop could be ATR based or % of current price\n",
    "                # current_high_since_entry = df_backtest.loc[active_trade['entry_bar_index']:idx, 'high'].max()\n",
    "                # potential_new_stop = current_high_since_entry * (1 - (stop_loss_pct * 1.5)) # Trail by 1.5x SL\n",
    "                # active_trade['stop_loss_price'] = max(active_trade['stop_loss_price'], potential_new_stop)\n",
    "                pass # Simplified for now, add more complex trailing if needed\n",
    "\n",
    "            # Check Stop Loss (using current bar's low)\n",
    "            if active_trade['type'] == 'long' and current_price_low <= active_trade['stop_loss_price']:\n",
    "                exit_price = active_trade['stop_loss_price'] \n",
    "                pnl = (exit_price - active_trade['entry_price_eff']) * active_trade['quantity']\n",
    "                pnl -= (active_trade['entry_price_eff'] * active_trade['quantity'] * fee_rate) # Entry fee already \"paid\" conceptually by reducing capital available or factored in cost basis\n",
    "                pnl -= (exit_price * active_trade['quantity'] * fee_rate) # Exit fee\n",
    "                \n",
    "                balance += pnl # P&L is directly added/subtracted\n",
    "                total_pnl_realized += pnl\n",
    "                trades_log.append({'entry_dt': active_trade['entry_dt'], 'exit_dt': current_dt, 'type': 'long', 'entry': active_trade['entry_price_eff'], 'exit': exit_price, 'qty': active_trade['quantity'], 'pnl': pnl, 'reason': 'StopLoss'})\n",
    "                logging.info(f\"STOP-LOSS @ {current_dt}: Long exited at {exit_price:.2f}. Qty: {active_trade['quantity']:.4f}. P&L: ${pnl:.2f}. Balance: ${balance:.2f}\")\n",
    "                if pnl > 0: num_wins += 1\n",
    "                else: num_losses += 1\n",
    "                active_trade = None; position_qty = 0; trade_closed_this_bar = True\n",
    "\n",
    "            # Check Take Profit (using current bar's high)\n",
    "            elif active_trade and not trade_closed_this_bar and active_trade['type'] == 'long' and current_price_high >= active_trade['take_profit_price']:\n",
    "                exit_price = active_trade['take_profit_price']\n",
    "                pnl = (exit_price - active_trade['entry_price_eff']) * active_trade['quantity']\n",
    "                pnl -= (active_trade['entry_price_eff'] * active_trade['quantity'] * fee_rate) \n",
    "                pnl -= (exit_price * active_trade['quantity'] * fee_rate) \n",
    "\n",
    "                balance += pnl\n",
    "                total_pnl_realized += pnl\n",
    "                trades_log.append({'entry_dt': active_trade['entry_dt'], 'exit_dt': current_dt, 'type': 'long', 'entry': active_trade['entry_price_eff'], 'exit': exit_price, 'qty': active_trade['quantity'], 'pnl': pnl, 'reason': 'TakeProfit'})\n",
    "                logging.info(f\"TAKE-PROFIT @ {current_dt}: Long exited at {exit_price:.2f}. Qty: {active_trade['quantity']:.4f}. P&L: ${pnl:.2f}. Balance: ${balance:.2f}\")\n",
    "                if pnl > 0: num_wins += 1\n",
    "                else: num_losses += 1 # Should be positive for TP\n",
    "                active_trade = None; position_qty = 0; trade_closed_this_bar = True\n",
    "        \n",
    "        # --- Check Entry Conditions (if no active trade) ---\n",
    "        # Ensure current bar's data isn't NaN for indicators\n",
    "        if not active_trade and not pd.isna(row[signal_col]) and \\\n",
    "           not pd.isna(row[rsi_col]) and not pd.isna(row[bb_lower_col]) and \\\n",
    "           not pd.isna(row[sma_short_col]) and not pd.isna(row[sma_long_col]):\n",
    "\n",
    "            if (row[signal_col] == 1 and # ML Buy Signal (Assuming 1 = Buy)\n",
    "                row[rsi_col] < 35 and    # Additional filter: RSI (configurable)\n",
    "                row['close'] < row[bb_lower_col] and # Additional filter: Close below BB_lower\n",
    "                row[sma_short_col] > row[sma_long_col]):   # Additional filter: Trend filter\n",
    "\n",
    "                entry_price_nominal = current_price_close # Typically use next bar's open, or current close for simplicity\n",
    "                entry_price_slippage = entry_price_nominal * (1 + slippage_rate)\n",
    "                \n",
    "                stop_loss_price_val = entry_price_slippage * (1 - stop_loss_pct)\n",
    "                take_profit_price_val = entry_price_slippage * (1 + take_profit_pct)\n",
    "\n",
    "                dollar_risk_per_unit = entry_price_slippage - stop_loss_price_val\n",
    "                if dollar_risk_per_unit <= 1e-6: # Avoid division by zero/tiny risk\n",
    "                    logging.warning(f\"Calculated dollar_risk_per_unit ({dollar_risk_per_unit:.4f}) is too small or zero. Skipping trade entry @ {current_dt}\")\n",
    "                    continue\n",
    "                \n",
    "                capital_to_risk = balance * risk_fraction_per_trade\n",
    "                quantity = capital_to_risk / dollar_risk_per_unit\n",
    "                \n",
    "                cost_of_trade = entry_price_slippage * quantity\n",
    "                entry_fee = cost_of_trade * fee_rate\n",
    "\n",
    "                if balance < cost_of_trade + entry_fee : # Check affordability\n",
    "                    logging.info(f\"Insufficient balance for trade @ {current_dt}. Need ${cost_of_trade + entry_fee:.2f}, Have ${balance:.2f}\")\n",
    "                    continue \n",
    "\n",
    "                # Execute Buy\n",
    "                # Balance is NOT reduced by cost_of_trade here. Fees are paid.\n",
    "                # The \"cost\" is that capital is now tied up in the asset.\n",
    "                balance -= entry_fee \n",
    "                \n",
    "                position_qty = quantity\n",
    "                active_trade = {\n",
    "                    'type': 'long', 'entry_dt': current_dt,\n",
    "                    'entry_price_nominal': entry_price_nominal,\n",
    "                    'entry_price_eff': entry_price_slippage, # Effective entry after slippage AND entry fee\n",
    "                    'quantity': quantity,\n",
    "                    'stop_loss_price': stop_loss_price_val,\n",
    "                    'take_profit_price': take_profit_price_val,\n",
    "                    'entry_bar_index': idx # Store index for potential later reference (e.g. calculating high since entry)\n",
    "                }\n",
    "                logging.info(f\"ENTRY @ {current_dt}: Long at {entry_price_slippage:.2f}. Qty: {quantity:.4f}. SL: {stop_loss_price_val:.2f}, TP: {take_profit_price_val:.2f}. Balance after fee: ${balance:.2f}\")\n",
    "        \n",
    "        # Update balance history (Mark-to-market equity)\n",
    "        current_equity = balance + (position_qty * current_price_close * (1 - fee_rate) if active_trade else 0) # Value of open position (less exit fee)\n",
    "        balance_history.append(current_equity)\n",
    "\n",
    "    # --- End of Loop: Liquidate any open positions ---\n",
    "    if active_trade:\n",
    "        final_close_price = df_backtest.iloc[-1][\"close\"]\n",
    "        logging.info(f\"End of backtest. Liquidating open {active_trade['type']} position of {active_trade['quantity']:.4f} at market close {final_close_price:.2f}\")\n",
    "        \n",
    "        exit_price = final_close_price * (1 - slippage_rate) # Assume slippage on market close\n",
    "        pnl = (exit_price - active_trade['entry_price_eff']) * active_trade['quantity']\n",
    "        pnl -= (active_trade['entry_price_eff'] * active_trade['quantity'] * fee_rate) \n",
    "        pnl -= (exit_price * active_trade['quantity'] * fee_rate)\n",
    "\n",
    "        balance += pnl\n",
    "        total_pnl_realized += pnl\n",
    "        trades_log.append({'entry_dt': active_trade['entry_dt'], 'exit_dt': df_backtest.iloc[-1][\"timestamp\"], 'type': 'long', 'entry': active_trade['entry_price_eff'], 'exit': exit_price, 'qty': active_trade['quantity'], 'pnl': pnl, 'reason': 'EndOfBacktest'})\n",
    "        if pnl > 0: num_wins += 1\n",
    "        else: num_losses += 1\n",
    "        logging.info(f\"   Final P&L on liquidation: ${pnl:.2f}. Final Balance: ${balance:.2f}\")\n",
    "\n",
    "    # --- Calculate Performance Metrics ---\n",
    "    net_profit = balance - initial_balance\n",
    "    total_trades_executed = num_wins + num_losses\n",
    "    win_rate_val = num_wins / total_trades_executed if total_trades_executed > 0 else 0\n",
    "    \n",
    "    # Sum of P&Ls for profit factor\n",
    "    sum_positive_pnl = sum(t['pnl'] for t in trades_log if t['pnl'] > 0)\n",
    "    sum_negative_pnl = abs(sum(t['pnl'] for t in trades_log if t['pnl'] < 0))\n",
    "    profit_factor_val = sum_positive_pnl / sum_negative_pnl if sum_negative_pnl > 0 else float('inf') if sum_positive_pnl > 0 else 0\n",
    "\n",
    "    # Max Drawdown Calculation from equity curve\n",
    "    equity_curve = pd.Series(balance_history)\n",
    "    peak_equity = equity_curve.cummax()\n",
    "    drawdown = (equity_curve - peak_equity) / peak_equity\n",
    "    max_dd_val = abs(drawdown.min()) if not drawdown.empty else 0\n",
    "            \n",
    "    logging.info(f\"\\n--- Backtest Results ---\")\n",
    "    logging.info(f\"Period: {df_backtest['timestamp'].iloc[0]} to {df_backtest['timestamp'].iloc[-1]}\")\n",
    "    logging.info(f\"Initial Balance: ${initial_balance:.2f}\")\n",
    "    logging.info(f\"Final Balance: ${balance:.2f}\")\n",
    "    logging.info(f\"Net Profit: ${net_profit:.2f} ({(net_profit/initial_balance)*100:.2f}%)\")\n",
    "    logging.info(f\"Total Realized P&L: ${total_pnl_realized:.2f}\")\n",
    "    logging.info(f\"Total Trades: {total_trades_executed}\")\n",
    "    logging.info(f\"Wins: {num_wins}, Losses: {num_losses}\")\n",
    "    logging.info(f\"Win Rate: {win_rate_val:.2%}\")\n",
    "    logging.info(f\"Profit Factor: {profit_factor_val:.2f}\")\n",
    "    logging.info(f\"Max Drawdown: {max_dd_val:.2%}\")\n",
    "    \n",
    "    results_summary = {\n",
    "        \"initial_balance\": initial_balance, \"final_balance\": balance, \"net_profit_dollars\": net_profit,\n",
    "        \"net_profit_pct\": (net_profit/initial_balance)*100 if initial_balance > 0 else 0,\n",
    "        \"total_realized_pnl\": total_pnl_realized,\n",
    "        \"total_trades\": total_trades_executed, \"wins\": num_wins, \"losses\": num_losses,\n",
    "        \"win_rate\": win_rate_val, \"profit_factor\": profit_factor_val, \"max_drawdown_pct\": max_dd_val,\n",
    "    }\n",
    "    return net_profit, results_summary, trades_log, balance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986c94f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Plotting Function\n",
    "\n",
    "Visualizes the price action, Bollinger Bands, RSI, and trading signals (from ML model or rules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trading_chart_with_signals(df_to_plot, signal_col_name=\"predicted_signal\",\n",
    "                                    rsi_thresholds_info=None, trades_df=None):\n",
    "    \"\"\"\n",
    "    Plots candlestick chart, BBands, SMAs, RSI with signals, and optionally trade entry/exit markers.\n",
    "    - rsi_thresholds_info: dict {'buy_threshold': val, 'sell_threshold': val} for plotting RSI lines.\n",
    "    - trades_df: DataFrame of trades from backtest log for plotting entry/exits.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Plotting chart. Data shape: {df_to_plot.shape}. Signal column: {signal_col_name}\")\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.05,\n",
    "        row_heights=[0.6, 0.2, 0.2], # Adjust row heights\n",
    "        subplot_titles=(\"Price Action with Bollinger Bands & Trades\", \"Volume\", \"RSI & Signals\")\n",
    "    )\n",
    "    \n",
    "    # --- Row 1: Candlestick, Bollinger Bands, Trade Markers ---\n",
    "    fig.add_trace(go.Candlestick(\n",
    "        x=df_to_plot[\"timestamp\"], open=df_to_plot[\"open\"], high=df_to_plot[\"high\"],\n",
    "        low=df_to_plot[\"low\"], close=df_to_plot[\"close\"], name=\"Price\"\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=df_to_plot[\"timestamp\"], y=df_to_plot[\"BB_hband\"], mode=\"lines\", name=\"BB Upper\", line=dict(color=\"red\", dash=\"dot\", width=1)), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df_to_plot[\"timestamp\"], y=df_to_plot[\"BB_mavg\"], mode=\"lines\", name=\"BB Mid\", line=dict(color=\"blue\", dash=\"dash\", width=1)), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df_to_plot[\"timestamp\"], y=df_to_plot[\"BB_lband\"], mode=\"lines\", name=\"BB Lower\", line=dict(color=\"green\", dash=\"dot\", width=1)), row=1, col=1)\n",
    "\n",
    "    # Plot SMA lines\n",
    "    if \"SMA_short\" in df_to_plot.columns:\n",
    "        fig.add_trace(go.Scatter(x=df_to_plot[\"timestamp\"], y=df_to_plot[\"SMA_short\"], mode=\"lines\", name=\"SMA Short\", line=dict(color=\"orange\", width=1)), row=1, col=1)\n",
    "    if \"SMA_long\" in df_to_plot.columns:\n",
    "        fig.add_trace(go.Scatter(x=df_to_plot[\"timestamp\"], y=df_to_plot[\"SMA_long\"], mode=\"lines\", name=\"SMA Long\", line=dict(color=\"purple\", width=1)), row=1, col=1)\n",
    "\n",
    "    # Plot Trade Entry/Exit Markers if trades_df is provided\n",
    "    if trades_df is not None and not trades_df.empty:\n",
    "        # Ensure datetime conversion if necessary\n",
    "        trades_df_copy = trades_df.copy()\n",
    "        if not pd.api.types.is_datetime64_any_dtype(trades_df_copy['entry_dt']):\n",
    "            trades_df_copy['entry_dt'] = pd.to_datetime(trades_df_copy['entry_dt'])\n",
    "        if not pd.api.types.is_datetime64_any_dtype(trades_df_copy['exit_dt']):\n",
    "            trades_df_copy['exit_dt'] = pd.to_datetime(trades_df_copy['exit_dt'])\n",
    "\n",
    "        winning_trades = trades_df_copy[trades_df_copy['pnl'] > 0]\n",
    "        losing_trades = trades_df_copy[trades_df_copy['pnl'] <= 0]\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=winning_trades['entry_dt'], y=winning_trades['entry'], mode=\"markers\", name=\"Win Entry\",\n",
    "            marker=dict(color=\"lime\", symbol=\"triangle-up\", size=10, line=dict(color='black', width=1))), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=winning_trades['exit_dt'], y=winning_trades['exit'], mode=\"markers\", name=\"Win Exit\",\n",
    "            marker=dict(color=\"lime\", symbol=\"circle\", size=8, line=dict(color='black', width=1))), row=1, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=losing_trades['entry_dt'], y=losing_trades['entry'], mode=\"markers\", name=\"Loss Entry\",\n",
    "            marker=dict(color=\"magenta\", symbol=\"triangle-down\", size=10, line=dict(color='black', width=1))), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=losing_trades['exit_dt'], y=losing_trades['exit'], mode=\"markers\", name=\"Loss Exit\",\n",
    "            marker=dict(color=\"magenta\", symbol=\"circle\", size=8, line=dict(color='black', width=1))), row=1, col=1)\n",
    "        \n",
    "        # Draw lines connecting entry and exit for each trade\n",
    "        for _, trade in trades_df_copy.iterrows():\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[trade['entry_dt'], trade['exit_dt']],\n",
    "                y=[trade['entry'], trade['exit']],\n",
    "                mode=\"lines\", showlegend=False,\n",
    "                line=dict(color=\"lime\" if trade['pnl'] > 0 else \"magenta\", width=1, dash=\"dash\")\n",
    "            ), row=1, col=1)\n",
    "\n",
    "\n",
    "    # --- Row 2: Volume ---\n",
    "    fig.add_trace(go.Bar(x=df_to_plot[\"timestamp\"], y=df_to_plot[\"volume\"], name=\"Volume\", marker_color='grey'), row=2, col=1)\n",
    "\n",
    "\n",
    "    # --- Row 3: RSI & Signals ---\n",
    "    fig.add_trace(go.Scatter(x=df_to_plot[\"timestamp\"], y=df_to_plot[\"RSI\"], mode=\"lines\", name=\"RSI\", line=dict(color=\"cyan\", width=2)), row=3, col=1)\n",
    "    \n",
    "    # Plot Buy Signals (e.g., signal_col_name == 1)\n",
    "    buy_signals = df_to_plot[df_to_plot[signal_col_name] == 1]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=buy_signals[\"timestamp\"], y=buy_signals[\"RSI\"], mode=\"markers\", name=\"Buy Signal (Model)\",\n",
    "        marker=dict(color=\"lime\", symbol=\"triangle-up\", size=9, line=dict(color='black',width=1))\n",
    "    ), row=3, col=1)\n",
    "    \n",
    "    # Plot Sell Signals (e.g., signal_col_name == 2)\n",
    "    sell_signals = df_to_plot[df_to_plot[signal_col_name] == 2]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sell_signals[\"timestamp\"], y=sell_signals[\"RSI\"], mode=\"markers\", name=\"Sell Signal (Model)\",\n",
    "        marker=dict(color=\"red\", symbol=\"triangle-down\", size=9, line=dict(color='black',width=1))\n",
    "    ), row=3, col=1)\n",
    "    \n",
    "    # Plot RSI Threshold Lines (from training or fixed)\n",
    "    if rsi_thresholds_info:\n",
    "        fig.add_hline(y=rsi_thresholds_info['sell_threshold'], line=dict(color='red', dash='dash', width=1), row=3, col=1,\n",
    "                      annotation_text=f\"Train Sell Thr: {rsi_thresholds_info['sell_threshold']:.2f}\", annotation_position=\"top right\")\n",
    "        fig.add_hline(y=rsi_thresholds_info['buy_threshold'], line=dict(color='lime', dash='dash', width=1), row=3, col=1,\n",
    "                      annotation_text=f\"Train Buy Thr: {rsi_thresholds_info['buy_threshold']:.2f}\", annotation_position=\"bottom right\")\n",
    "    else: # Fallback to fixed lines if no dynamic thresholds provided\n",
    "        fig.add_hline(y=65, line=dict(color='red', dash='dash', width=1), row=3, col=1, annotation_text=\"RSI Overbought (65)\", annotation_position=\"top right\")\n",
    "        fig.add_hline(y=35, line=dict(color='lime', dash='dash', width=1), row=3, col=1, annotation_text=\"RSI Oversold (35)\", annotation_position=\"bottom right\")\n",
    "        \n",
    "    # --- Layout & Styling ---\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Trading Strategy Analysis ({df_to_plot['timestamp'].iloc[0].date()} to {df_to_plot['timestamp'].iloc[-1].date()})\",\n",
    "        height=1200, template=\"plotly_dark\",\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Price (USDT)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Volume\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"RSI\", row=3, col=1)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686d569",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Main Execution Orchestrator\n",
    "\n",
    "This `main()` function ties together all the steps: data fetching, feature engineering, ML data preparation, model training, evaluation, signal generation, backtesting, and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d995f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_trading_pipeline():\n",
    "    logging.info(\"--- Starting Main Trading Pipeline ---\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    SYMBOL = \"BTCUSDT\"\n",
    "    INTERVAL = \"4h\" # Shorter interval for more data points for ML\n",
    "    DATA_LIMIT = 1000 # Number of candles to fetch for the entire process\n",
    "    # For fetching by date range:\n",
    "    # START_DATE_STR = \"1 Jan, 2022\"\n",
    "    # END_DATE_STR = \"1 Jan, 2023\"\n",
    "    \n",
    "    # Features to be used by the ML model\n",
    "    MODEL_FEATURES = [\"close\", \"RSI\", \"BB_hband\", \"BB_lband\", \"BB_mavg\", \"SMA_short\", \"SMA_long\", \"volume_pct_change\"]\n",
    "    # Ensure these are calculated in calculate_technical_indicators and present after NaN drop\n",
    "    \n",
    "    RSI_LOWER_QUANTILE = 0.15 # For defining \"buy\" based on RSI (more signals)\n",
    "    RSI_UPPER_QUANTILE = 0.85 # For defining \"sell\" based on RSI\n",
    "\n",
    "    TEST_SET_SIZE_ML = 0.20 # 20% for ML model testing\n",
    "    RANDOM_ITER_XGB = 20    # For RandomizedSearchCV (keep low for speed)\n",
    "    \n",
    "    APPLY_SMOTE = True         # Whether to apply SMOTE for class imbalance\n",
    "    USE_SAMPLE_WEIGHTS_XGB = True # Whether to use sample_weights in XGBoost (can be used with or without SMOTE)\n",
    "\n",
    "    # Backtest parameters\n",
    "    INITIAL_BALANCE_BT = 10000\n",
    "    FEE_RATE_BT = 0.00075 # Binance spot trading fee (maker/taker)\n",
    "    SLIPPAGE_RATE_BT = 0.0005 # 0.05% slippage\n",
    "    RISK_FRACTION_BT = 0.01 # Risk 1% of equity per trade\n",
    "    STOP_LOSS_PCT_BT = 0.02 # 2% stop loss\n",
    "    TAKE_PROFIT_PCT_BT = 0.05 # 4% take profit (2:1 R:R)\n",
    "\n",
    "\n",
    "    # --- 1. Fetch Data ---\n",
    "    # df_raw = fetch_crypto_data_client(binance_client, symbol=SYMBOL, interval=INTERVAL, start_str=START_DATE_STR, end_str=END_DATE_STR)\n",
    "    df_raw = fetch_crypto_data_client(binance_client, symbol=SYMBOL, interval=INTERVAL, limit=DATA_LIMIT)\n",
    "    if df_raw is None or df_raw.empty:\n",
    "        logging.error(\"Failed to fetch initial data. Exiting pipeline.\")\n",
    "        return\n",
    "    logging.info(f\"Raw data fetched: {df_raw.shape[0]} rows from {df_raw['timestamp'].min()} to {df_raw['timestamp'].max()}\")\n",
    "\n",
    "    # --- 2. Calculate Technical Indicators (Features) ---\n",
    "    df_featurized = calculate_technical_indicators(df_raw.copy()) # Pass a copy\n",
    "    if df_featurized.empty:\n",
    "        logging.error(\"DataFrame empty after feature calculation. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Prepare Data for ML (Split, Correct Labeling) ---\n",
    "    X_train, X_test, y_train, y_test, train_rsi_thresholds = prepare_ml_data(\n",
    "        df_featurized,\n",
    "        model_input_features=MODEL_FEATURES,\n",
    "        target_rsi_series_name=\"RSI\", # Assuming target is based on RSI\n",
    "        test_set_size=TEST_SET_SIZE_ML,\n",
    "        lower_quantile=RSI_LOWER_QUANTILE, # Passed to get_rsi_quantile_thresholds via prepare_ml_data\n",
    "        upper_quantile=RSI_UPPER_QUANTILE  # Passed to get_rsi_quantile_thresholds via prepare_ml_data\n",
    "    )\n",
    "    if X_train is None or y_train is None:\n",
    "        logging.error(\"ML data preparation failed (X_train or y_train is None). Exiting.\")\n",
    "        return\n",
    "    logging.info(f\"ML Data Prepared: X_train shape {X_train.shape}, y_train_dist: {dict(pd.Series(y_train).value_counts())}\")\n",
    "\n",
    "\n",
    "    # --- 4. (Optional) Apply SMOTE to Training Data ---\n",
    "    X_train_final = X_train.copy()\n",
    "    y_train_final = y_train.copy()\n",
    "    if APPLY_SMOTE:\n",
    "        X_train_final, y_train_final = apply_smote_to_train_data(X_train, y_train)\n",
    "    \n",
    "\n",
    "    # --- 5. Train XGBoost Model ---\n",
    "    ml_model = None\n",
    "    if len(np.unique(y_train_final)) < 2:\n",
    "        logging.warning(\"Not enough unique classes in final training data to train model. Skipping training.\")\n",
    "    else:\n",
    "        ml_model = train_xgboost_model(X_train_final, y_train_final,\n",
    "                                       n_random_iter=RANDOM_ITER_XGB,\n",
    "                                       use_sample_weights=USE_SAMPLE_WEIGHTS_XGB)\n",
    "    if ml_model is None:\n",
    "        logging.error(\"ML model training failed. Exiting pipeline.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # --- 6. Evaluate ML Model on Test Set ---\n",
    "    logging.info(\"\\n--- Evaluating ML Model on Unseen Test Set ---\")\n",
    "    evaluate_ml_model(ml_model, X_test, y_test, dataset_name=\"ML Hold-Out Test\")\n",
    "    advanced_ml_evaluation(ml_model, X_train_final, y_train_final, cv_folds=3) # CV on the data used for training (SMOTE'd or not)\n",
    "    \n",
    "\n",
    "    # To reconstruct df_cleaned if not returned directly:\n",
    "    df_cleaned_for_prediction = df_featurized.dropna(subset=MODEL_FEATURES + [\"RSI\"]).copy() # Align with what was split\n",
    "    if df_cleaned_for_prediction.empty:\n",
    "        logging.error(\"Cleaned DataFrame for prediction signals is empty. Cannot backtest.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"\\n--- Generating ML Predictions for Backtesting on Cleaned Featurized Data ---\")\n",
    "    df_for_backtesting_input = apply_trained_model_to_df(df_cleaned_for_prediction, ml_model, MODEL_FEATURES)\n",
    "    \n",
    "    if \"predicted_signal\" not in df_for_backtesting_input.columns or df_for_backtesting_input[\"predicted_signal\"].isnull().all():\n",
    "        logging.error(\"Failed to generate 'predicted_signal' for backtesting. Exiting.\")\n",
    "        return\n",
    "    logging.info(f\"'predicted_signal' distribution for backtest: {dict(df_for_backtesting_input['predicted_signal'].value_counts(dropna=False))}\")\n",
    "\n",
    "    # --- 8. Run Backtesting ---\n",
    "    logging.info(\"\\n--- Running Backtest with ML Model Signals ---\")\n",
    "    # Pass all necessary column names if they differ from defaults in backtest_ml_strategy\n",
    "    net_profit_bt, backtest_summary, trades_history_bt, balance_curve_bt = backtest_ml_strategy(\n",
    "        df_for_backtesting_input, # This df has features and 'predicted_signal'\n",
    "        initial_balance=INITIAL_BALANCE_BT,\n",
    "        fee_rate=FEE_RATE_BT,\n",
    "        slippage_rate=SLIPPAGE_RATE_BT,\n",
    "        risk_fraction_per_trade=RISK_FRACTION_BT,\n",
    "        stop_loss_pct=STOP_LOSS_PCT_BT,\n",
    "        take_profit_pct=TAKE_PROFIT_PCT_BT,\n",
    "        signal_col=\"predicted_signal\", # Explicitly state the signal column\n",
    "        rsi_col=\"RSI\", bb_lower_col=\"BB_lband\", # Match column names from `ta` library\n",
    "        sma_short_col=\"SMA_short\", sma_long_col=\"SMA_long\"\n",
    "    )\n",
    "    logging.info(f\"Backtesting complete. Net Profit: ${net_profit_bt:.2f}\")\n",
    "    logging.info(f\"Backtest Summary: {backtest_summary}\")\n",
    "\n",
    "    # Convert trades log to DataFrame for easier plotting/analysis\n",
    "    trades_df_bt = pd.DataFrame(trades_history_bt)\n",
    "\n",
    "    # --- 9. Plotting ---\n",
    "    logging.info(\"\\n--- Plotting Results ---\")\n",
    "    # Plot on the same data used for backtesting input for consistency\n",
    "    plot_trading_chart_with_signals(\n",
    "        df_for_backtesting_input,\n",
    "        signal_col_name=\"predicted_signal\",\n",
    "        rsi_thresholds_info=train_rsi_thresholds, # Show thresholds learned from training\n",
    "        trades_df=trades_df_bt # Pass the trades log to plot markers\n",
    "    )\n",
    "    \n",
    "    # Plot equity curve\n",
    "    if balance_curve_bt:\n",
    "        fig_equity = go.Figure()\n",
    "        fig_equity.add_trace(go.Scatter(x=list(range(len(balance_curve_bt))), y=balance_curve_bt, mode='lines', name='Equity Curve'))\n",
    "        fig_equity.update_layout(title='Backtest Equity', xaxis_title='Trade/Time Step', yaxis_title='Balance', template='plotly_dark')\n",
    "        fig_equity.show()\n",
    "\n",
    "    logging.info(\"--- Main Trading Pipeline Finished ---\")\n",
    "\n",
    "# --- Run the Pipeline ---\n",
    "if __name__ == \"__main__\":\n",
    "    main_trading_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
